{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import model\n",
    "import data\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "import model\n",
    "from trainlogger import TrainLogger\n",
    "from gensim.models.word2vec import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    print('num batches:',nbatch)\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1, 200).transpose(1,0).contiguous()\n",
    "    return data\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len])\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "total_batches = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wem_model = KeyedVectors.load(\"./data/wikitext-2/wikitext-2a.w2v\")\n",
    "corpus = data.Corpus(\"./data/wikitext-2/\", None, wem_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "train_data = batchify(corpus.train, 20)\n",
    "val_data = batchify(corpus.valid, 10)\n",
    "test_data = batchify(corpus.test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.valid.shape)\n",
    "print(val_data.shape)\n",
    "# CHECK IF the 2nd value in the first batch is the same as the 2nd value in the training set\n",
    "all(v == 1 for v in [torch.eq(corpus.train[1],train_data[1,0])][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = 'LSTM'\n",
    "ntokens = corpus.dict_size()\n",
    "emsize = 200\n",
    "nhid = 500\n",
    "nlayers = 2\n",
    "dropout = 0.5\n",
    "lr = 0.5\n",
    "bptt = 5\n",
    "clip = 0.25\n",
    "\n",
    "rnn_model = model.RNNModel(modelType, nhid, nlayers, dropout, wem_model)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_tensor(wem, tensor):\n",
    "    if isinstance(tensor,torch.FloatTensor):\n",
    "        data = tensor\n",
    "    elif isinstance(tensor,torch.autograd.variable.Variable):\n",
    "        data = tensor.data\n",
    "    else:\n",
    "        print('ksss') # but could also just be numpy array\n",
    "    if data.shape[1] != len(wem.wv.syn0[0]): # better way to get size of embedding...\n",
    "        print(\"Sizes don't match: tensor:(%s) - embedding:(%s)\" % (data.shape[1], len(wem.wv.syn0)))\n",
    "    return ' '.join([\n",
    "        '|'.join([res[0] + ('\\n' if res[0] == '<eos>' else '') for res in wem.wv.similar_by_vector(i.numpy(),1)])\n",
    "        for i in data\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train():\n",
    "# Turn on training mode which enables dropout.\n",
    "rnn_model.train()\n",
    "total_loss = 0\n",
    "hidden = rnn_model.init_hidden(batch_size)\n",
    "\n",
    "num_batches = 5\n",
    "# for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "for batch, i in enumerate(range(0, bptt * num_batches, bptt)):\n",
    "#     print('batch',batch)\n",
    "    t_data, targets = get_batch(train_data, i)\n",
    "#     get input vector shape:  shape: bptt * emb_size\n",
    "#     print(t_data.data[:,0,:].shape) \n",
    "#     To assure we grabbing the right thing:\n",
    "#     print(\"most similar word to the first word-vector in training data:\")\n",
    "#     print(wem_model.wv.similar_by_vector(t_data.data[0,0,:].numpy(),topn=1)[0][0])\n",
    "#     introspect that vector and the sum of that vector\n",
    "#     print(t_data.data[0,0,:5], t_data.data[0,0,:].sum())\n",
    "#     print words of the whole first batch, derived from the train-data tensors...\n",
    "    input_words = get_words_for_tensor(wem_model,t_data.data[:,0,:])\n",
    "    print('\"\"\" INPUT\\n%s\\n\"\"\"' % input_words)\n",
    "#     print words of the whole first batch targets, derived from the train-data tensors...   \n",
    "    target_words = get_words_for_tensor(wem_model,targets.data[:,0,:])\n",
    "    print('\"\"\" TARGET\\n%s\\n\"\"\"' % target_words)\n",
    "#     print('training_data_batch',t_data.data.shape)\n",
    "#     print('targets',targets.data.shape)\n",
    "#     # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "#     # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "\n",
    "    hidden = repackage_hidden(hidden)\n",
    "    rnn_model.zero_grad()\n",
    "    output, hidden = rnn_model(t_data, hidden)\n",
    "    output_words = get_words_for_tensor(wem_model,output.data[:,0,:])\n",
    "    print('\"\"\" OUTPUT\\n%s\\n\"\"\"' % output_words)\n",
    "#     print('output',output.data.shape)\n",
    "#     print(get_words_for_tensor(wem_model,output.data[:,0,:]))\n",
    "    loss = criterion(output,targets)\n",
    "    print(\"loss:\",loss.data)\n",
    "    loss.backward()\n",
    "\n",
    "#     # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#     torch.nn.utils.clip_grad_norm(rnn_model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "#     if batch % args.log_interval == 0 and batch > 0:\n",
    "#         cur_loss = total_loss[0] / args.log_interval\n",
    "#         elapsed = time.time() - start_time\n",
    "#         print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "#                 'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "#             epoch, batch, len(train_data) // args.bptt, lr,\n",
    "#             elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "#         total_loss = 0\n",
    "#         logger.log(total_batches, math.exp(cur_loss))\n",
    "#         start_time = time.time()\n",
    "#     total_batches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_params = list(rnn_model.rnn.parameters())\n",
    "type(rnn_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_word = 'Chronicles'\n",
    "print(wem_model.wv[test_word][:5])\n",
    "index = wem_model.wv.vocab[test_word].index\n",
    "print('index ',index)\n",
    "print(wem_model.wv.syn0[index][:5])\n",
    "# np.sum(wem_model.wv.syn0[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
